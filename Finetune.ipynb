{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 7877435,
          "sourceType": "datasetVersion",
          "datasetId": 4623055
        },
        {
          "sourceId": 7882963,
          "sourceType": "datasetVersion",
          "datasetId": 4626971
        },
        {
          "sourceId": 7884686,
          "sourceType": "datasetVersion",
          "datasetId": 4628299
        },
        {
          "sourceId": 7884806,
          "sourceType": "datasetVersion",
          "datasetId": 4628402
        },
        {
          "sourceId": 7904281,
          "sourceType": "datasetVersion",
          "datasetId": 4642658
        },
        {
          "sourceId": 7914172,
          "sourceType": "datasetVersion",
          "datasetId": 4649908
        },
        {
          "sourceId": 7915850,
          "sourceType": "datasetVersion",
          "datasetId": 4651198
        },
        {
          "sourceId": 7916602,
          "sourceType": "datasetVersion",
          "datasetId": 4651725
        },
        {
          "sourceId": 7917143,
          "sourceType": "datasetVersion",
          "datasetId": 4652079
        },
        {
          "sourceId": 7917560,
          "sourceType": "datasetVersion",
          "datasetId": 4652380
        },
        {
          "sourceId": 7928569,
          "sourceType": "datasetVersion",
          "datasetId": 4659919
        }
      ],
      "dockerImageVersionId": 30665,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Finetunning"
      ],
      "metadata": {
        "id": "1SwMjXVKuZVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "import pandas as pd\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "from transformers import EarlyStoppingCallback\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "\n",
        "# Load your dataset\n",
        "data = pd.read_excel('')\n",
        "\n",
        "train_data, temp_data = train_test_split(data, test_size=0.2, stratify=data['Type'], random_state=42)\n",
        "val_data, test_data = train_test_split(temp_data, test_size=0.5, stratify=temp_data['Type'], random_state=42)\n",
        "\n",
        "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
        "tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
        "model.config.gradient_checkpointing = True\n",
        "\n",
        "\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"all_mbart_English_unify_PUNCT\",\n",
        "    gradient_accumulation_steps=2,\n",
        "    logging_steps=100,\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=6,\n",
        "    logging_dir=\"/kaggle/working/logs\",\n",
        "    evaluation_strategy='steps',\n",
        "    eval_steps=100,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"loss\",\n",
        ")\n",
        "\n",
        "\n",
        "class DataFrameDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Return a tuple of strings\n",
        "        return self.data.iloc[idx][\"Indonesian statement\"], self.data.iloc[idx][\"English statement\"]\n",
        "\n",
        "class CustomDataCollator(DataCollatorForSeq2Seq):\n",
        "    def __init__(self, tokenizer, model):\n",
        "        super().__init__(tokenizer, model=model)\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        input_texts, target_texts = zip(*batch)\n",
        "        return self.tokenizer.prepare_seq2seq_batch(src_texts=input_texts, tgt_texts=target_texts, padding='longest', max_length=100, return_tensors='pt')\n",
        "data_collator = CustomDataCollator(tokenizer, model)\n",
        "\n",
        "train_dataset = DataFrameDataset(train_data)\n",
        "eval_dataset = DataFrameDataset(val_data)\n",
        "\n",
        "# Initialize the trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n",
        "\n",
        "# Save the model\n",
        "trainer.save_model(\"\")\n",
        "\n",
        "# Save the tokenizer\n",
        "tokenizer.save_pretrained('')\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-21T12:08:46.918496Z",
          "iopub.execute_input": "2024-03-21T12:08:46.919267Z",
          "iopub.status.idle": "2024-03-21T12:43:53.375054Z",
          "shell.execute_reply.started": "2024-03-21T12:08:46.919231Z",
          "shell.execute_reply": "2024-03-21T12:43:53.374039Z"
        },
        "trusted": true,
        "colab": {
          "referenced_widgets": [
            "2c8d3d88089143c9bcfa3b8cdc5d84d0",
            "dc898f38c322478482807564a29d1ee2",
            "af3ac767e7a14ab3a72b9f1ff24b5819",
            "9eb2ec3f2afe464db2766571c9071878",
            "09741b10423140d09e9cd7a939fb6308",
            "05e498cd0c224eb0995e25eb905cf444"
          ]
        },
        "id": "_V5TJ6WAlu08",
        "outputId": "95ca693b-3b1d-4f6c-ec9e-3bf7a8c1beb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "2024-03-21 12:09:04.198030: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-21 12:09:04.198199: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-21 12:09:04.499225: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2c8d3d88089143c9bcfa3b8cdc5d84d0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/2.44G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dc898f38c322478482807564a29d1ee2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "generation_config.json:   0%|          | 0.00/261 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "af3ac767e7a14ab3a72b9f1ff24b5819"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/529 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9eb2ec3f2afe464db2766571c9071878"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "09741b10423140d09e9cd7a939fb6308"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/649 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "05e498cd0c224eb0995e25eb905cf444"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:",
          "output_type": "stream"
        },
        {
          "output_type": "stream",
          "name": "stdin",
          "text": "  ········································\n"
        },
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "wandb version 0.16.4 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Tracking run with wandb version 0.16.3"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Run data is saved locally in <code>/kaggle/working/wandb/run-20240321_120958-8b2mv7z7</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Syncing run <strong><a href='https://wandb.ai/paradisez2001/huggingface/runs/8b2mv7z7' target=\"_blank\">solar-lion-21</a></strong> to <a href='https://wandb.ai/paradisez2001/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View project at <a href='https://wandb.ai/paradisez2001/huggingface' target=\"_blank\">https://wandb.ai/paradisez2001/huggingface</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run at <a href='https://wandb.ai/paradisez2001/huggingface/runs/8b2mv7z7' target=\"_blank\">https://wandb.ai/paradisez2001/huggingface/runs/8b2mv7z7</a>"
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4018: FutureWarning: \n`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n`__call__` method to prepare your inputs and targets.\n\nHere is a short example:\n\nmodel_inputs = tokenizer(src_texts, text_target=tgt_texts, ...)\n\nIf you either need to use different keyword arguments for the source and target texts, you should do two calls like\nthis:\n\nmodel_inputs = tokenizer(src_texts, ...)\nlabels = tokenizer(text_target=tgt_texts, ...)\nmodel_inputs[\"labels\"] = labels[\"input_ids\"]\n\nSee the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\nFor a more complete example, see the implementation of `prepare_seq2seq_batch`.\n\n  warnings.warn(formatted_warning, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3892: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='1020' max='1020' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1020/1020 33:12, Epoch 6/6]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>1.808500</td>\n      <td>0.181855</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.120900</td>\n      <td>0.088513</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.057600</td>\n      <td>0.057008</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.032800</td>\n      <td>0.040060</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.021500</td>\n      <td>0.033800</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.014200</td>\n      <td>0.027958</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.011400</td>\n      <td>0.024497</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.008300</td>\n      <td>0.022429</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.006300</td>\n      <td>0.020221</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.005500</td>\n      <td>0.020172</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_eos_token_id': 2}\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4018: FutureWarning: \n`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n`__call__` method to prepare your inputs and targets.\n\nHere is a short example:\n\nmodel_inputs = tokenizer(src_texts, text_target=tgt_texts, ...)\n\nIf you either need to use different keyword arguments for the source and target texts, you should do two calls like\nthis:\n\nmodel_inputs = tokenizer(src_texts, ...)\nlabels = tokenizer(text_target=tgt_texts, ...)\nmodel_inputs[\"labels\"] = labels[\"input_ids\"]\n\nSee the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\nFor a more complete example, see the implementation of `prepare_seq2seq_batch`.\n\n  warnings.warn(formatted_warning, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3892: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_eos_token_id': 2}\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4018: FutureWarning: \n`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of HuggingFace Transformers. Use the regular\n`__call__` method to prepare your inputs and targets.\n\nHere is a short example:\n\nmodel_inputs = tokenizer(src_texts, text_target=tgt_texts, ...)\n\nIf you either need to use different keyword arguments for the source and target texts, you should do two calls like\nthis:\n\nmodel_inputs = tokenizer(src_texts, ...)\nlabels = tokenizer(text_target=tgt_texts, ...)\nmodel_inputs[\"labels\"] = labels[\"input_ids\"]\n\nSee the documentation of your specific tokenizer for more details on the specific arguments to the tokenizer of choice.\nFor a more complete example, see the implementation of `prepare_seq2seq_batch`.\n\n  warnings.warn(formatted_warning, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3892: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\nThere were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 200, 'early_stopping': True, 'num_beams': 5, 'forced_eos_token_id': 2}\n",
          "output_type": "stream"
        },
        {
          "execution_count": 1,
          "output_type": "execute_result",
          "data": {
            "text/plain": "('/kaggle/working/all_mbart_English_unify_PUNCT/tokenizer_config.json',\n '/kaggle/working/all_mbart_English_unify_PUNCT/special_tokens_map.json',\n '/kaggle/working/all_mbart_English_unify_PUNCT/sentencepiece.bpe.model',\n '/kaggle/working/all_mbart_English_unify_PUNCT/added_tokens.json',\n '/kaggle/working/all_mbart_English_unify_PUNCT/tokenizer.json')"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
        "device = 'cuda'\n",
        "\n",
        "tokenizer.src_lang = \"fa_IR\"\n",
        "\n",
        "source = \"امیلی یک مکانیک و جورج یک وکیل است. او با ماشین کار می کند.\"\n",
        "encoded_source = tokenizer(source, return_tensors=\"pt\").to(device)\n",
        "\n",
        "generated_tokens = model.generate(\n",
        "    **encoded_source,\n",
        "    forced_bos_token_id=tokenizer.lang_code_to_id[\"en_XX\"],\n",
        "    max_length=512\n",
        ")\n",
        "\n",
        "print(tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0])\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-21T14:47:27.120398Z",
          "iopub.execute_input": "2024-03-21T14:47:27.120783Z",
          "iopub.status.idle": "2024-03-21T14:47:27.698566Z",
          "shell.execute_reply.started": "2024-03-21T14:47:27.120752Z",
          "shell.execute_reply": "2024-03-21T14:47:27.697553Z"
        },
        "trusted": true,
        "id": "pPWYNhF7lu0-",
        "outputId": "708e6774-f061-406f-d573-5dc96d78b5b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Emily is a mechanic and George is a lawyer. She works with cars.\n",
          "output_type": "stream"
        }
      ]
    }
  ]
}